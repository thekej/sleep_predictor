{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import argparse\n",
    "import h5py\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from models import SleepPredictionMLP\n",
    "from models import SleepPredictionCNN\n",
    "from models import SleepPredictionSeq\n",
    "from dataloader import get_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Obj(dict):\n",
    "    \"\"\"Converts dicts to objects.\n",
    "    \"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        if name in self:\n",
    "            return self[name]\n",
    "        else:\n",
    "            raise AttributeError(\"No such attribute: \" + name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "    def __delattr__(self, name):\n",
    "        if name in self:\n",
    "            del self[name]\n",
    "        else:\n",
    "            raise AttributeError(\"No such attribute: \" + name)\n",
    "\n",
    "    def merge(self, other, overwrite=True):\n",
    "        for name in other:\n",
    "            if overwrite or name not in self:\n",
    "                self[name] = other[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Setting up seeds.\n",
    "    torch.cuda.manual_seed(args['--seed'])\n",
    "    torch.manual_seed(args['--seed'])\n",
    "    \n",
    "    # Create model directory.\n",
    "    if not os.path.exists(args['--model-dir']):\n",
    "        os.makedirs(args['--model-dir'])\n",
    "\n",
    "    # Config logging.\n",
    "    log_format = '%(levelname)-8s %(message)s'\n",
    "    logfile = os.path.join(args['--model-dir'], 'train.log')\n",
    "    logging.basicConfig(filename=logfile, level=logging.INFO, format=log_format)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler())\n",
    "    #logging.info(json.dumps(args))\n",
    "\n",
    "    # Save the arguments.\n",
    "    with open(os.path.join(args['--model-dir'], 'args.json'), 'w') as args_file:\n",
    "        json.dump(args, args_file)\n",
    "\n",
    "    # Build data loader.\n",
    "    logging.info(\"Building data loader...\")\n",
    "    dataset_size = get_dataset_size(args['--dataset'])\n",
    "    train_set = random.sample(range(dataset_size), int(dataset_size * 0.9))\n",
    "    data_loader = get_data_loader(args['--dataset'], args['--labels-data'], args['--batch-size'], shuffle=True,\n",
    "                                  num_workers=args['--num-workers'], model_type=args['--mode'],\n",
    "                                  indices=train_set)\n",
    "    val_data_loader = get_data_loader(args['--dataset'], args['--labels-data'], args['--batch-size'], shuffle=False,\n",
    "                                      num_workers=args['--num-workers'], model_type=args['--mode'],\n",
    "                                      indices=list(set(range(dataset_size)) - set(train_set)))\n",
    "    logging.info(\"Done\")\n",
    "\n",
    "    # Build the models\n",
    "    logging.info(\"Building Sleep Stage Predictor...\")\n",
    "    if args['--mode'] == 'mlp':\n",
    "        model = SleepPredictionMLP(1250, 256, num_classes=args['--output-size'],\n",
    "                                   num_layers=2, dropout_p=0.0, w_norm=False)\n",
    "    elif args['--mode'] == 'cnn':\n",
    "        model = SleepPredictionCNN(output_size=args['--output-size'])\n",
    "    else:\n",
    "        model = SleepPredictionSeq(output_size=args['--output-size'])\n",
    "    pre = 0\n",
    "    if args['--pretrained']:\n",
    "        model.load_state_dict(torch.load(args['--model-path']))\n",
    "        pre = int(args['--model-path'].split('-')[1].split('.')[0])\n",
    "    logging.info(\"Done\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    # Loss and Optimizer.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        criterion.cuda()        \n",
    "\n",
    "    # Parameters to train.\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=0.99, weight_decay = 0.0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['--learning-rate'], weight_decay=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer=optimizer, mode='min',\n",
    "                                  factor=0.1, patience=args['--patience'],\n",
    "                                  verbose=True, min_lr=1e-6)\n",
    "\n",
    "    # Train the Models.\n",
    "    total_steps = len(data_loader) * args['--num-epochs']\n",
    "    start_time = time.time()\n",
    "    \n",
    "    n_steps = 0\n",
    "    for epoch in range(args['--num-epochs']):\n",
    "        l = 0.0\n",
    "        a = 0.0\n",
    "        start_time = time.time()\n",
    "        for i, (conditions, eegs, labels) in enumerate(data_loader):\n",
    "            n_steps += 1\n",
    "            # Set mini-batch dataset.\n",
    "            if torch.cuda.is_available():\n",
    "                eegs = eegs.cuda()\n",
    "                conditions = conditions.cuda()\n",
    "                labels = labels.cuda()            \n",
    "            # Forward.\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs = model(eegs, conditions)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backprop and optimize.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Eval now.\n",
    "            #if (n_steps % args.eval_every_n_steps == 0):\n",
    "            #    run_eval(model, val_data_loader, criterion,\n",
    "            #             args, epoch, scheduler)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            a += (preds == labels).sum().item()\n",
    "            l += loss.item()\n",
    "\n",
    "            if (i+1) % args['--log-step'] == 0:\n",
    "                logging.info('Time: %.2f Epoch [%d/%d], step [%d/%d], Train loss: %.4f, Train accuracy: %.4f' % (\n",
    "                    time.time() - start_time, epoch, args['--num-epochs'], i+1, len(data_loader), \n",
    "                    loss.item(), (preds == labels).sum().item() / float(args['--batch-size'])))\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Save the models.\n",
    "            #if (i+1) % args.save_step == 0:\n",
    "            #    torch.save(model.state_dict(),\n",
    "            #               os.path.join(args.model_dir, \n",
    "            #                            'model-%d-%d.pkl' %(epoch+1, i+1)))\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(args['--model-dir'],\n",
    "                   'model-%d.pkl' % (epoch+1+pre)))\n",
    "\n",
    "        # Evaluation and learning rate updates.\n",
    "        logging.info('Epoch [%d/%d], Train loss: %.4f, Train accuracy: %.4f' % (\n",
    "                    epoch, args['--num-epochs'], l / len(data_loader), a / int(dataset_size * 0.9)))\n",
    "        run_eval(model, val_data_loader, criterion, args, epoch, scheduler)\n",
    "\n",
    "    # Save the final model.\n",
    "    torch.save(model.state_dict(),os.path.join(args['--model-dir'],'model.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {}\n",
    "args_dict['--model-dir'] = 'weights/mlp/'\n",
    "args_dict['--model-path'] = 'weights/resnet/model.pkl'\n",
    "args_dict['--dataset'] = 'data/processed_train_dataset.hdf5'\n",
    "args_dict['--labels-data'] = 'data/y_train_2.csv'\n",
    "\n",
    "# Session parameters.\n",
    "args_dict['--log-step'] = 10\n",
    "args_dict['--save-step'] = 1000\n",
    "args_dict['--eval-steps'] = None\n",
    "args_dict['--eval-every-n-steps'] = 1000\n",
    "args_dict['--eval-all'] = True\n",
    "args_dict['--num-epochs'] = 10\n",
    "args_dict['--batch-size'] = 16\n",
    "args_dict['--num-workers'] = 8\n",
    "args_dict['--learning-rate'] = 0.001\n",
    "args_dict['--patience'] = 0\n",
    "args_dict['--data-size'] = 100\n",
    "args_dict['--seed'] = 1\n",
    "\n",
    "# Model parameters.\n",
    "args_dict['--mode'] = 'mlp'\n",
    "args_dict['--output-size'] = 3\n",
    "args_dict['--pretrained'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_size(dataset):\n",
    "    annos = h5py.File(dataset, 'r')\n",
    "    size = annos['mlp'].shape[0]\n",
    "    annos.close()\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--num-epochs',\n",
       " '--pretrained',\n",
       " '--seed',\n",
       " '--eval-all',\n",
       " '--mode',\n",
       " '--data-size',\n",
       " '--model-path',\n",
       " '--output-size',\n",
       " '--log-step',\n",
       " '--eval-steps',\n",
       " '--dataset',\n",
       " '--eval-every-n-steps',\n",
       " '--model-dir',\n",
       " '--save-step',\n",
       " '--learning-rate',\n",
       " '--num-workers',\n",
       " '--batch-size',\n",
       " '--patience',\n",
       " '--labels-data']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab all the training parameters.\n",
    "args = Dict2Obj(args_dict)\n",
    "type(args)\n",
    "list(args.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building data loader...\n",
      "Done\n",
      "Building Sleep Stage Predictor...\n",
      "Done\n",
      "Time: 9.53 Epoch [0/10], step [10/14717], Train loss: 2.0331, Train accuracy: 0.0000\n",
      "Time: 4.55 Epoch [0/10], step [20/14717], Train loss: 1.4200, Train accuracy: 0.0000\n",
      "Time: 4.54 Epoch [0/10], step [30/14717], Train loss: 1.5190, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [40/14717], Train loss: 1.1964, Train accuracy: 0.0000\n",
      "Time: 8.85 Epoch [0/10], step [50/14717], Train loss: 1.6673, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [60/14717], Train loss: 1.4077, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [70/14717], Train loss: 1.3757, Train accuracy: 0.0000\n",
      "Time: 4.55 Epoch [0/10], step [80/14717], Train loss: 1.3761, Train accuracy: 0.0000\n",
      "Time: 8.83 Epoch [0/10], step [90/14717], Train loss: 1.1389, Train accuracy: 0.0000\n",
      "Time: 4.51 Epoch [0/10], step [100/14717], Train loss: 1.2444, Train accuracy: 0.0000\n",
      "Time: 4.48 Epoch [0/10], step [110/14717], Train loss: 1.1523, Train accuracy: 0.0000\n",
      "Time: 4.50 Epoch [0/10], step [120/14717], Train loss: 1.3506, Train accuracy: 0.0000\n",
      "Time: 8.92 Epoch [0/10], step [130/14717], Train loss: 1.1316, Train accuracy: 0.0000\n",
      "Time: 4.47 Epoch [0/10], step [140/14717], Train loss: 2.2143, Train accuracy: 0.0000\n",
      "Time: 4.50 Epoch [0/10], step [150/14717], Train loss: 0.9713, Train accuracy: 0.0000\n",
      "Time: 4.50 Epoch [0/10], step [160/14717], Train loss: 1.2866, Train accuracy: 0.0000\n",
      "Time: 8.85 Epoch [0/10], step [170/14717], Train loss: 1.2525, Train accuracy: 0.0000\n",
      "Time: 4.57 Epoch [0/10], step [180/14717], Train loss: 1.1668, Train accuracy: 0.0000\n",
      "Time: 4.46 Epoch [0/10], step [190/14717], Train loss: 1.2512, Train accuracy: 0.0000\n",
      "Time: 4.52 Epoch [0/10], step [200/14717], Train loss: 1.2234, Train accuracy: 0.0000\n",
      "Time: 8.90 Epoch [0/10], step [210/14717], Train loss: 1.2285, Train accuracy: 0.0000\n",
      "Time: 4.48 Epoch [0/10], step [220/14717], Train loss: 1.0791, Train accuracy: 0.0000\n",
      "Time: 4.50 Epoch [0/10], step [230/14717], Train loss: 0.8204, Train accuracy: 0.0000\n",
      "Time: 4.56 Epoch [0/10], step [240/14717], Train loss: 1.2359, Train accuracy: 0.0000\n",
      "Time: 9.00 Epoch [0/10], step [250/14717], Train loss: 0.8112, Train accuracy: 0.0000\n",
      "Time: 4.50 Epoch [0/10], step [260/14717], Train loss: 1.0026, Train accuracy: 0.0000\n",
      "Time: 4.54 Epoch [0/10], step [270/14717], Train loss: 1.0786, Train accuracy: 0.0000\n",
      "Time: 4.46 Epoch [0/10], step [280/14717], Train loss: 1.5894, Train accuracy: 0.0000\n",
      "Time: 8.91 Epoch [0/10], step [290/14717], Train loss: 1.4629, Train accuracy: 0.0000\n",
      "Time: 4.46 Epoch [0/10], step [300/14717], Train loss: 1.2551, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [310/14717], Train loss: 0.9544, Train accuracy: 0.0000\n",
      "Time: 4.46 Epoch [0/10], step [320/14717], Train loss: 0.9827, Train accuracy: 0.0000\n",
      "Time: 8.89 Epoch [0/10], step [330/14717], Train loss: 0.7630, Train accuracy: 0.0000\n",
      "Time: 4.55 Epoch [0/10], step [340/14717], Train loss: 1.1390, Train accuracy: 0.0000\n",
      "Time: 4.52 Epoch [0/10], step [350/14717], Train loss: 0.9749, Train accuracy: 0.0000\n",
      "Time: 4.54 Epoch [0/10], step [360/14717], Train loss: 1.2306, Train accuracy: 0.0000\n",
      "Time: 9.05 Epoch [0/10], step [370/14717], Train loss: 1.0812, Train accuracy: 0.0000\n",
      "Time: 4.50 Epoch [0/10], step [380/14717], Train loss: 0.8485, Train accuracy: 0.0000\n",
      "Time: 4.56 Epoch [0/10], step [390/14717], Train loss: 1.4285, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [400/14717], Train loss: 0.9595, Train accuracy: 0.0000\n",
      "Time: 9.03 Epoch [0/10], step [410/14717], Train loss: 0.9571, Train accuracy: 0.0000\n",
      "Time: 4.54 Epoch [0/10], step [420/14717], Train loss: 1.4244, Train accuracy: 0.0000\n",
      "Time: 4.58 Epoch [0/10], step [430/14717], Train loss: 0.9781, Train accuracy: 0.0000\n",
      "Time: 4.59 Epoch [0/10], step [440/14717], Train loss: 1.3182, Train accuracy: 0.0000\n",
      "Time: 8.98 Epoch [0/10], step [450/14717], Train loss: 1.0820, Train accuracy: 0.0000\n",
      "Time: 4.55 Epoch [0/10], step [460/14717], Train loss: 1.2227, Train accuracy: 0.0000\n",
      "Time: 4.58 Epoch [0/10], step [470/14717], Train loss: 1.1997, Train accuracy: 0.0000\n",
      "Time: 4.55 Epoch [0/10], step [480/14717], Train loss: 1.0962, Train accuracy: 0.0000\n",
      "Time: 9.05 Epoch [0/10], step [490/14717], Train loss: 1.1576, Train accuracy: 0.0000\n",
      "Time: 4.54 Epoch [0/10], step [500/14717], Train loss: 1.1837, Train accuracy: 0.0000\n",
      "Time: 4.56 Epoch [0/10], step [510/14717], Train loss: 1.1876, Train accuracy: 0.0000\n",
      "Time: 4.63 Epoch [0/10], step [520/14717], Train loss: 1.1780, Train accuracy: 0.0000\n",
      "Time: 8.97 Epoch [0/10], step [530/14717], Train loss: 1.2546, Train accuracy: 0.0000\n",
      "Time: 4.58 Epoch [0/10], step [540/14717], Train loss: 1.1286, Train accuracy: 0.0000\n",
      "Time: 4.56 Epoch [0/10], step [550/14717], Train loss: 0.9134, Train accuracy: 0.0000\n",
      "Time: 4.54 Epoch [0/10], step [560/14717], Train loss: 1.2582, Train accuracy: 0.0000\n",
      "Time: 9.20 Epoch [0/10], step [570/14717], Train loss: 0.8095, Train accuracy: 0.0000\n",
      "Time: 4.56 Epoch [0/10], step [580/14717], Train loss: 0.7775, Train accuracy: 0.0000\n",
      "Time: 4.57 Epoch [0/10], step [590/14717], Train loss: 0.9387, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [600/14717], Train loss: 0.7734, Train accuracy: 0.0000\n",
      "Time: 9.00 Epoch [0/10], step [610/14717], Train loss: 1.3625, Train accuracy: 0.0000\n",
      "Time: 4.56 Epoch [0/10], step [620/14717], Train loss: 0.9985, Train accuracy: 0.0000\n",
      "Time: 4.59 Epoch [0/10], step [630/14717], Train loss: 0.8710, Train accuracy: 0.0000\n",
      "Time: 4.61 Epoch [0/10], step [640/14717], Train loss: 1.0564, Train accuracy: 0.0000\n",
      "Time: 9.01 Epoch [0/10], step [650/14717], Train loss: 1.0074, Train accuracy: 0.0000\n",
      "Time: 4.65 Epoch [0/10], step [660/14717], Train loss: 1.1502, Train accuracy: 0.0000\n",
      "Time: 4.57 Epoch [0/10], step [670/14717], Train loss: 1.1457, Train accuracy: 0.0000\n",
      "Time: 4.59 Epoch [0/10], step [680/14717], Train loss: 1.1658, Train accuracy: 0.0000\n",
      "Time: 9.05 Epoch [0/10], step [690/14717], Train loss: 1.0177, Train accuracy: 0.0000\n",
      "Time: 4.62 Epoch [0/10], step [700/14717], Train loss: 1.0724, Train accuracy: 0.0000\n",
      "Time: 4.59 Epoch [0/10], step [710/14717], Train loss: 1.1031, Train accuracy: 0.0000\n",
      "Time: 4.54 Epoch [0/10], step [720/14717], Train loss: 1.1636, Train accuracy: 0.0000\n",
      "Time: 9.03 Epoch [0/10], step [730/14717], Train loss: 1.0855, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [740/14717], Train loss: 1.0636, Train accuracy: 0.0000\n",
      "Time: 4.53 Epoch [0/10], step [750/14717], Train loss: 0.9966, Train accuracy: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3a2a40925975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-bede19918708>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconditions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meegs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mn_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Set mini-batch dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/thekej/QBot/qbot/env/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/thekej/QBot/qbot/env/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/thekej/QBot/qbot/env/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/queues.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
